{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM Inference Server with MLflow\n",
    "\n",
    "In this notebook, we will create a custom MLflow `PythonModel` (PyFunc model) and serve it on our local machine. We will then use MLflow AI Gateway to access the model via an OpenAI-compatible endpoint that works with the OpenAI Python SDK.\n",
    "\n",
    "Our custom model will randomly select a model and a system prompt, hinting at how we could further develop this approach into a more sophisticated model evaluation system.\n",
    "\n",
    "## Model Setup\n",
    "\n",
    "First, we will load our environment variables, import the necessary libraries, and set up our system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "Note that, at present, the schema of ChatModel is not compatible with Gateway, so we need to use a PythonModel instead.\n",
    "\n",
    "See [here](https://github.com/mlflow/mlflow/blob/0866e4c4b06a598144c0771592fbdaf7f0fadd6d/mlflow/gateway/providers/mlflow.py#L145C1-L170C1) for details on the schema AI Gateway sends to the MLflow model server.\n",
    "\n",
    "AI Gateway sends a request to the model server formatted as follows:\n",
    "\n",
    "```\n",
    "# Example request to MLflow REST API for chat:\n",
    "# {\n",
    "#     \"inputs\": [\"question\"],\n",
    "#     \"params\": [\"temperature\": 0.2],\n",
    "# }\n",
    "```\n",
    "\n",
    "Some notes:\n",
    "- AI Gateway expects messages formatted in an OpenAI-compatible way.\n",
    "- Those messages are translated to the above format, which is what is sent to the model server.\n",
    "- This will not send a message history. Only the most recent message is sent.\n",
    "\n",
    "Here is the code for our custom model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_chat_pyfunc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_chat_pyfunc.py\n",
    "\n",
    "import mlflow.pyfunc\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from mlflow.models import set_model\n",
    "\n",
    "class TestModel(PythonModel):\n",
    "    def __init__(self):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\") and os.path.exists(\".env\"):\n",
    "            load_dotenv()\n",
    "        self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.gemini_client = OpenAI(\n",
    "            api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "            base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "        )\n",
    "    \n",
    "    def _select_model(self):\n",
    "        return \"gpt-4o-mini\" if random.random() < 0.5 else \"gemini-1.5-flash-latest\"\n",
    "\n",
    "    def _select_sys_prompt(self):\n",
    "        return random.choice([\n",
    "            \"You are a helpful assistant.\",\n",
    "            \"You are watching TV and don't want to be bothered.\"\n",
    "        ])\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Get the content string from the input\n",
    "        content = model_input[0]\n",
    "        \n",
    "        # Format messages with system prompt\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self._select_sys_prompt()},\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ]\n",
    "\n",
    "        # Get response from model\n",
    "        model = self._select_model()\n",
    "        client = self.openai_client if model == \"gpt-4o-mini\" else self.gemini_client\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.8,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        # Return string response in list as expected by AI Gateway.\n",
    "        # It will be translated to a dictionary by Gateway.\n",
    "        return [response.choices[0].message.content]\n",
    "\n",
    "set_model(TestModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "Let's load the model and make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/11 14:44:49 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"Invalid type for \\'messages[1].content[0]\\': expected an object, but got a string instead.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': \\'messages[1].content[0]\\', \\'code\\': \\'invalid_type\\'}}'). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n",
      "2024/12/11 14:44:53 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "2024/12/11 14:44:53 INFO mlflow.models.model: Found the following environment variables used during model inference: [GEMINI_API_KEY, OPENAI_API_KEY]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serve format test: [\"I'm currently focused on my show, but feel free to ask your question, and I'll do my best to help!\"]\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "code_path = \"my_chat_pyfunc.py\"\n",
    "\n",
    "# Input example should match how it arrives from AI Gateway\n",
    "input_example = [\"Hello, I have a question for you.\"]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        \"my_chat_pyfunc\",\n",
    "        python_model=code_path,\n",
    "        input_example=input_example,\n",
    "    )\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "# Test the model with array input (how it arrives via serve)\n",
    "result = loaded_model.predict(input_example)\n",
    "print(\"Serve format test:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve the Custom Model Locally\n",
    "\n",
    "We can serve the model with `mlflow models serve` CLI command. We use:\n",
    "\n",
    "`export $(cat .env | xargs) && mlflow models serve -m file:///Users/daniel.liden/git/devrel-examples/notebooks/mlflow/mlruns/0/1a5f86514a6c4cfbac6db9aa38c44fda/artifacts/test_model -p 5002 --env-manager local`\n",
    "\n",
    "So the model is now being served at `http://127.0.0.1:5002`.\n",
    "\n",
    "For full compatibility with the OpenAI SDK, we need to use the MLFlow AI Gateway, which will provide an openai-compatible endpoint. Here's how we configure it to work with mlflow serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gateway.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile gateway.yaml\n",
    "endpoints:\n",
    "  - name: chat\n",
    "    endpoint_type: llm/v1/chat\n",
    "    model:\n",
    "      provider: mlflow-model-serving\n",
    "      name: my_chat_pyfunc\n",
    "      config:\n",
    "        model_server_url: http://127.0.0.1:5002\n",
    "        openai_api_key: $OPENAI_API_KEY\n",
    "        gemini_api_key: $GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start both servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the model server (run in a separate terminal):\n",
    "# export $(cat .env | grep -e OPENAI_API_KEY -e GEMINI_API_KEY | xargs) && mlflow models serve -m models:/test_model/latest -p 5002 --env-manager local\n",
    "\n",
    "# Start the gateway (run in a separate terminal):\n",
    "# mlflow gateway start --config-path gateway.yaml --port 7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can query our custom model using the OpenAI SDKâ€”with the caveat noted above. We cannot send a message history, only the most recent message.\n",
    "\n",
    "Note that we also did not configure our model to handle streaming or inference parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id=None, choices=[Choice(finish_reason=None, index=0, logprobs=None, message=ChatCompletionMessage(content='(No response, eyes glued to the TV screen.)\\n', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733950135, model='my_chat_pyfunc', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=None, prompt_tokens=None, total_tokens=None, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client pointing to the local gateway\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:7000/v1\",\n",
    "    api_key=\"not-needed\"\n",
    ")\n",
    "\n",
    "# Format the data as a dictionary with the required schema\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Make the request\n",
    "response = client.chat.completions.create(\n",
    "    model=\"chat\",\n",
    "    messages=messages,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we can use the MLflow REST API directly (without invoking the Gateway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'predictions': [\"Of course! I'm here to help. What's your question?\"]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Test direct model server\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:5002/invocations\",\n",
    "    json={\n",
    "        \"inputs\": [\"Hello, I have a question for you.\"]\n",
    "    },\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
