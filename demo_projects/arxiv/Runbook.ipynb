{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Arxiv Demo Runbook\n",
                "\n",
                "This notebook guides you through setting up and running the Arxiv Knowledge Assistant demo.\n",
                "\n",
                "## Prerequisites\n",
                "* Unity Catalog enabled workspace\n",
                "* Permissions to create Catalogs/Schemas/Volumes/Tables\n",
                "* Permissions to create Agents"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment\n",
                "Install dependencies and configure catalog settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install arxiv databricks-sdk streamlit\n",
                "%restart_python"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from databricks.sdk import WorkspaceClient\n",
                "\n",
                "# Add src to path so we can import arxiv_demo\n",
                "sys.path.append(os.path.abspath(\"src\"))\n",
                "\n",
                "# Helper to auto-detect warehouse\n",
                "def get_warehouse_id():\n",
                "    w = WorkspaceClient()\n",
                "    # List all warehouses\n",
                "    warehouses = list(w.warehouses.list())\n",
                "    if not warehouses:\n",
                "        return \"\" \n",
                "    \n",
                "    # Prefer running ones\n",
                "    running = [wh for wh in warehouses if wh.state == \"RUNNING\"]\n",
                "    if running:\n",
                "        return running[0].id\n",
                "        \n",
                "    # Fallback to first available\n",
                "    return warehouses[0].id\n",
                "\n",
                "detected_wh = get_warehouse_id()\n",
                "\n",
                "# Configuration Widgets\n",
                "dbutils.widgets.text(\"catalog\", \"arxiv_demo\", \"Catalog\")\n",
                "dbutils.widgets.text(\"schema\", \"main\", \"Schema\")\n",
                "dbutils.widgets.text(\"volume\", \"pdfs\", \"Volume\")\n",
                "dbutils.widgets.text(\"warehouse_id\", detected_wh, \"SQL Warehouse ID\")\n",
                "\n",
                "# Set env vars for the demo code to pick up\n",
                "os.environ[\"ARXIV_CATALOG\"] = dbutils.widgets.get(\"catalog\")\n",
                "os.environ[\"ARXIV_SCHEMA\"] = dbutils.widgets.get(\"schema\")\n",
                "os.environ[\"ARXIV_VOLUME\"] = dbutils.widgets.get(\"volume\")\n",
                "os.environ[\"DATABRICKS_WAREHOUSE_ID\"] = dbutils.widgets.get(\"warehouse_id\")\n",
                "\n",
                "print(f\"Configuring for {os.environ['ARXIV_CATALOG']}.{os.environ['ARXIV_SCHEMA']}\")\n",
                "print(f\"Using Warehouse ID: {os.environ['DATABRICKS_WAREHOUSE_ID']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd86c129",
            "metadata": {},
            "outputs": [],
            "source": [
                "from arxiv_demo.setup import DatabricksSetup\n",
                "from databricks.sdk import WorkspaceClient\n",
                "\n",
                "catalog = os.environ[\"ARXIV_CATALOG\"]\n",
                "warehouse_id = os.environ[\"DATABRICKS_WAREHOUSE_ID\"]\n",
                "\n",
                "# Pre-check: Ensure Catalog exists (setup.py expects it)\n",
                "w = WorkspaceClient()\n",
                "print(f\"Ensuring Catalog '{catalog}' exists...\")\n",
                "try:\n",
                "    w.catalogs.get(catalog)\n",
                "except Exception:\n",
                "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
                "\n",
                "\n",
                "# Run Setup (Schema, Volume, Tables)\n",
                "setup = DatabricksSetup()\n",
                "setup.setup_all(warehouse_id)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Ingest Golden Set Papers\n",
                "Download seminal LLM Agent papers (ReAct, Reflexion, etc.) and upload them to the UC Volume."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from arxiv_demo.ingestion import ArxivIngestion\n",
                "\n",
                "# Initialize ingestion (uses env vars set above)\n",
                "ingestion = ArxivIngestion()\n",
                "\n",
                "# Golden Set IDs\n",
                "GOLDEN_SET_IDS = [\n",
                "    \"2210.03629\", \"2303.11366\", \"2305.04091\", \"2304.08354\", \"2305.16291\"\n",
                "]\n",
                "\n",
                "print(\"Ingesting golden set papers...\")\n",
                "# Note: we import logic from the script or just invoke ingestion directly \n",
                "# Re-implementing simplified logic here for visibility in notebook\n",
                "\n",
                "import arxiv\n",
                "from arxiv_demo.ingestion import PaperMetadata\n",
                "\n",
                "client = arxiv.Client()\n",
                "search = arxiv.Search(id_list=GOLDEN_SET_IDS)\n",
                "papers = []\n",
                "\n",
                "for result in client.results(search):\n",
                "    p = PaperMetadata(\n",
                "        arxiv_id=result.entry_id.split(\"/\")[-1],\n",
                "        title=result.title,\n",
                "        authors=[a.name for a in result.authors],\n",
                "        abstract=result.summary,\n",
                "        published=result.published.isoformat(),\n",
                "        updated=result.updated.isoformat(),\n",
                "        categories=result.categories,\n",
                "        pdf_url=result.pdf_url\n",
                "    )\n",
                "    papers.append(p)\n",
                "\n",
                "print(f\"Found {len(papers)} papers. Uploading...\")\n",
                "ingestion.download_and_upload(papers, delay_seconds=10.0)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "03232497",
            "metadata": {},
            "source": [
                "### Parse Papers\n",
                "Parse the uploaded PDFs to extract text content. This is required for Key Information Extraction (KIE) to work."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28291bb5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run parsing script\n",
                "%run scripts/parse_golden_set.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Manual Step: Create Knowledge Assistant\n",
                "\n",
                "1. Go to **Agents** > **Create Agent**.\n",
                "2. Select **Knowledge Source**: Unity Catalog Volume (`arxiv_demo.main.pdfs`).\n",
                "3. Name it: `arxiv-papers`.\n",
                "4. Description: `An assistant that answers questions based on Arxiv papers.`\n",
                "5. Data Description: `A collection of academic papers from Arxiv covering various topics in computer science and AI.`\n",
                "6. Deploy and copy the **Serving Endpoint Name** (e.g. `agents_arxiv-papers`).\n",
                "7. Paste it below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dbutils.widgets.text(\"ka_endpoint\", \"agents_arxiv-papers\", \"KA Endpoint (After Creation)\")\n",
                "os.environ[\"KA_ENDPOINT\"] = dbutils.widgets.get(\"ka_endpoint\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluating the Assistant\n",
                "\n",
                "Now that the Knowledge Assistant is created, let's create the evaluation dataset and verify its quality.\n",
                "\n",
                "Create the table `arxiv_demo.main.eval_questions` for use in the KA Evaluation UI."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the script logic to populate table from evaluation_dataset.json\n",
                "%run scripts/create_eval_table"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Run Evaluation\n",
                "1. Go to your **Knowledge Assistant** (`arxiv-papers`).\n",
                "2. Click the **Improve Quality** tab.\n",
                "3. Select **Evaluation Set**: `arxiv_demo.main.eval_questions`.\n",
                "4. Click **Start Evaluation**."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Manual Step: Create KIE Agent\n",
                "\n",
                "The App uses a specialized agent to extract structured data from papers. Let's create it.\n",
                "\n",
                "1. Go to **Agents** > **Create Agent**.\n",
                "2. Select **Pattern**: Key Information Extraction.\n",
                "3. Name it: `arxiv-kie`.\n",
                "4. Configure **Schema** using this JSON Definition:\n",
                "\n",
                "```json\n",
                "{\n",
                "  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
                "  \"title\": \"Generated Schema\",\n",
                "  \"type\": \"object\",\n",
                "  \"properties\": {\n",
                "    \"affiliation\": {\n",
                "      \"description\": \"The \\\"affiliation\\\" field must contain the name of the organization, institution, or company with which the authors are associated. This information should be extracted as a string and may include department names, university names, or corporate entities. Ensure that the extracted content is precise and accurately reflects the authors' affiliations as stated in the source document.\",\n",
                "      \"anyOf\": [\n",
                "        {\n",
                "          \"type\": \"string\"\n",
                "        },\n",
                "        {\n",
                "          \"type\": \"null\"\n",
                "        }\n",
                "      ]\n",
                "    },\n",
                "    \"contributions\": {\n",
                "      \"description\": \"The \\\"contributions\\\" field must contain an array of strings that explicitly list the contributions made by the authors to the dataset or research presented in the document. Each entry in the array should clearly articulate a specific contribution, such as data collection, analysis, or writing, and should not include vague or general statements. If no contributions are provided, this field should be set to null.\",\n",
                "      \"anyOf\": [\n",
                "        {\n",
                "          \"type\": \"array\",\n",
                "          \"items\": {\n",
                "            \"type\": \"string\"\n",
                "          }\n",
                "        },\n",
                "        {\n",
                "          \"type\": \"null\"\n",
                "        }\n",
                "      ]\n",
                "    },\n",
                "    \"authors\": {\n",
                "      \"description\": \"The \\\"authors\\\" field must contain an array of strings, each representing the full name of an author associated with the dataset or research work. The names should be formatted as \\\"First Last\\\" without any titles or affiliations included. This field is required to accurately attribute contributions to the respective authors in the context of the dataset.\",\n",
                "      \"anyOf\": [\n",
                "        {\n",
                "          \"type\": \"array\",\n",
                "          \"items\": {\n",
                "            \"type\": \"string\"\n",
                "          }\n",
                "        },\n",
                "        {\n",
                "          \"type\": \"null\"\n",
                "        }\n",
                "      ]\n",
                "    },\n",
                "    \"title\": {\n",
                "      \"description\": \"The \\\"title\\\" field must contain the complete title of the document or work being referenced. It should be a string that accurately reflects the main subject or focus of the content, without any abbreviations or alterations. Ensure that the title is extracted as it appears in the source material, maintaining proper capitalization and punctuation.\",\n",
                "      \"anyOf\": [\n",
                "        {\n",
                "          \"type\": \"string\"\n",
                "        },\n",
                "        {\n",
                "          \"type\": \"null\"\n",
                "        }\n",
                "      ]\n",
                "    },\n",
                "    \"methodology\": {\n",
                "      \"type\": \"string\",\n",
                "      \"description\": \"The \\\"methodology\\\" field must describe the specific research methods, experimental designs, techniques, or approaches used to conduct the study. This includes data collection procedures, model architectures, training strategies, evaluation protocols, and any novel technical contributions to the research process itself.\"\n",
                "    },\n",
                "    \"limitations\": {\n",
                "      \"type\": \"array\",\n",
                "      \"description\": \"The \\\"limitations\\\" field must contain an array of strings listing the acknowledged weaknesses, constraints, and boundaries of the research. This includes scope restrictions, potential biases in data or methods, scenarios where the approach may fail, computational requirements, and areas identified for future improvement.\",\n",
                "      \"items\": {\n",
                "        \"type\": \"string\"\n",
                "      }\n",
                "    },\n",
                "    \"topics\": {\n",
                "      \"type\": \"array\",\n",
                "      \"description\": \"The \\\"topics\\\" field must list the main subject areas, research themes, and technical domains covered by the paper. This includes specific tasks addressed (e.g., question answering, code generation), model types (e.g., transformer, diffusion), and application areas (e.g., healthcare, robotics).\",\n",
                "      \"items\": {\n",
                "        \"type\": \"string\"\n",
                "      }\n",
                "    }\n",
                "  },\n",
                "  \"required\": [\n",
                "    \"affiliation\",\n",
                "    \"contributions\",\n",
                "    \"authors\",\n",
                "    \"title\",\n",
                "    \"methodology\",\n",
                "    \"limitations\",\n",
                "    \"topics\"\n",
                "  ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "5. Deploy and copy the **Serving Endpoint Name**.\n",
                "6. Paste it below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dbutils.widgets.text(\"kie_endpoint\", \"agents_arxiv-kie\", \"KIE Endpoint (After Creation)\")\n",
                "os.environ[\"KIE_ENDPOINT\"] = dbutils.widgets.get(\"kie_endpoint\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Run the Curator App\n",
                "Run the Streamlit app to search for new papers, extract information using KIE, and add them to your Knowledge Assistant.\n",
                "\n",
                "You can also chat with your assistant directly in the new **Chat** tab!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from streamlit.web.cli import main\n",
                "import sys\n",
                "\n",
                "# Fake args for streamlit\n",
                "sys.argv = [\"streamlit\", \"run\", \"app/main.py\"]\n",
                "main()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Deploy as Databricks App\n",
                "\n",
                "Ready to share your curator app? We can deploy it as a managed **Databricks App** using Databricks Asset Bundles (DABs).\n",
                "\n",
                "The project already includes `databricks.yml` and `app.yaml` configured for this deployment.\n",
                "\n",
                "**Note**: This requires the Databricks CLI to be installed and authenticated in your environment OR configured in your `.databrickscfg`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Deploy the app using DABs\n",
                "!databricks bundle deploy"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
