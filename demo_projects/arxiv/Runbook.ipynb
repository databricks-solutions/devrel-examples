{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Arxiv Demo Runbook\n",
                "\n",
                "This notebook guides you through setting up and running the Arxiv Knowledge Assistant demo.\n",
                "\n",
                "## Prerequisites\n",
                "* Unity Catalog enabled workspace\n",
                "* Permissions to create Catalogs/Schemas/Volumes/Tables\n",
                "* Permissions to create Agents"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment\n",
                "Install dependencies and configure catalog settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install arxiv databricks-sdk streamlit\n",
                "dbutils.library.restartPython()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from databricks.sdk import WorkspaceClient\n",
                "\n",
                "# Add src to path so we can import arxiv_demo\n",
                "sys.path.append(os.path.abspath(\"src\"))\n",
                "\n",
                "# Helper to auto-detect warehouse\n",
                "def get_warehouse_id():\n",
                "    w = WorkspaceClient()\n",
                "    # List all warehouses\n",
                "    warehouses = list(w.warehouses.list())\n",
                "    if not warehouses:\n",
                "        return \"\" \n",
                "    \n",
                "    # Prefer running ones\n",
                "    running = [wh for wh in warehouses if wh.state == \"RUNNING\"]\n",
                "    if running:\n",
                "        return running[0].id\n",
                "        \n",
                "    # Fallback to first available\n",
                "    return warehouses[0].id\n",
                "\n",
                "detected_wh = get_warehouse_id()\n",
                "\n",
                "# Configuration Widgets\n",
                "dbutils.widgets.text(\"catalog\", \"arxiv_demo\", \"Catalog\")\n",
                "dbutils.widgets.text(\"schema\", \"main\", \"Schema\")\n",
                "dbutils.widgets.text(\"volume\", \"pdfs\", \"Volume\")\n",
                "dbutils.widgets.text(\"warehouse_id\", detected_wh, \"SQL Warehouse ID\")\n",
                "\n",
                "# Set env vars for the demo code to pick up\n",
                "os.environ[\"ARXIV_CATALOG\"] = dbutils.widgets.get(\"catalog\")\n",
                "os.environ[\"ARXIV_SCHEMA\"] = dbutils.widgets.get(\"schema\")\n",
                "os.environ[\"ARXIV_VOLUME\"] = dbutils.widgets.get(\"volume\")\n",
                "os.environ[\"DATABRICKS_WAREHOUSE_ID\"] = dbutils.widgets.get(\"warehouse_id\")\n",
                "\n",
                "print(f\"Configuring for {os.environ['ARXIV_CATALOG']}.{os.environ['ARXIV_SCHEMA']}\")\n",
                "print(f\"Using Warehouse ID: {os.environ['DATABRICKS_WAREHOUSE_ID']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from arxiv_demo.setup import DatabricksSetup\n",
                "from databricks.sdk import WorkspaceClient\n",
                "\n",
                "catalog = os.environ[\"ARXIV_CATALOG\"]\n",
                "warehouse_id = os.environ[\"DATABRICKS_WAREHOUSE_ID\"]\n",
                "\n",
                "# Pre-check: Ensure Catalog exists (setup.py expects it)\n",
                "w = WorkspaceClient()\n",
                "print(f\"Ensuring Catalog '{catalog}' exists...\")\n",
                "try:\n",
                "    w.catalogs.get(catalog)\n",
                "except Exception:\n",
                "    print(f\"Creating catalog {catalog}...\")\n",
                "    w.catalogs.create(name=catalog)\n",
                "\n",
                "# Run Setup (Schema, Volume, Tables)\n",
                "setup = DatabricksSetup()\n",
                "setup.setup_all(warehouse_id)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Ingest Golden Set Papers\n",
                "Download seminal LLM Agent papers (ReAct, Reflexion, etc.) and upload them to the UC Volume."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from arxiv_demo.ingestion import ArxivIngestion\n",
                "\n",
                "# Initialize ingestion (uses env vars set above)\n",
                "ingestion = ArxivIngestion()\n",
                "\n",
                "# Golden Set IDs\n",
                "GOLDEN_SET_IDS = [\n",
                "    \"2210.03629\", \"2303.11366\", \"2305.04091\", \"2304.08354\", \"2305.16291\"\n",
                "]\n",
                "\n",
                "print(\"Ingesting golden set papers...\")\n",
                "# Note: we import logic from the script or just invoke ingestion directly \n",
                "# Re-implementing simplified logic here for visibility in notebook\n",
                "\n",
                "import arxiv\n",
                "from arxiv_demo.ingestion import PaperMetadata\n",
                "\n",
                "client = arxiv.Client()\n",
                "search = arxiv.Search(id_list=GOLDEN_SET_IDS)\n",
                "papers = []\n",
                "\n",
                "for result in client.results(search):\n",
                "    p = PaperMetadata(\n",
                "        arxiv_id=result.entry_id.split(\"/\")[-1],\n",
                "        title=result.title,\n",
                "        authors=[a.name for a in result.authors],\n",
                "        abstract=result.summary,\n",
                "        published=result.published.isoformat(),\n",
                "        updated=result.updated.isoformat(),\n",
                "        categories=result.categories,\n",
                "        pdf_url=result.pdf_url\n",
                "    )\n",
                "    papers.append(p)\n",
                "\n",
                "print(f\"Found {len(papers)} papers. Uploading...\")\n",
                "ingestion.download_and_upload(papers, delay_seconds=10.0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Manual Step: Create Knowledge Assistant\n",
                "\n",
                "1. Go to **Agents** > **Create Agent**.\n",
                "2. Select **Knowledge Source**: Unity Catalog Volume (`arxiv_demo.main.pdfs`).\n",
                "3. Name it: `arxiv-papers`.\n",
                "4. Deploy and copy the **Serving Endpoint Name** (e.g. `agents_arxiv-papers`).\n",
                "5. Paste it below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dbutils.widgets.text(\"ka_endpoint\", \"agents_arxiv-papers\", \"KA Endpoint (After Creation)\")\n",
                "os.environ[\"KA_ENDPOINT\"] = dbutils.widgets.get(\"ka_endpoint\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Manual Step: Create KIE Agent\n",
                "\n",
                "1. Go to **Agents** > **Create Agent**.\n",
                "2. Select **Pattern**: Key Information Extraction.\n",
                "3. Name it: `arxiv-kie`.\n",
                "4. Configure **Schema** using this JSON Definition:\n",
                "\n",
                "```json\n",
                "{\n",
                "  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
                "  \"title\": \"Generated Schema\",\n",
                "  \"type\": \"object\",\n",
                "  \"properties\": {\n",
                "    \"affiliation\": {\n",
                "      \"description\": \"The \\\"affiliation\\\" field must contain the name of the organization, institution, or company with which the authors are associated. This information should be extracted as a string and may include department names, university names, or corporate entities. Ensure that the extracted content is precise and accurately reflects the authors' affiliations as stated in the source document.\",\n",
                "      \"anyOf\": [\n",
                "        {\n",
                "          \"type\": \"string\"\n",
                "        },\n",
                "        {\n",
                "          \"type\": \"null\"\n",
                "        }\n",
                "      ]\n",
                "    },\n",
                "    \"contributions\": {\n",
                "      \"description\": \"The \\\"contributions\\\" field must contain an array of strings that explicitly list the contributions made by the authors to the dataset or research presented in the document. Each entry in the array should clearly articulate a specific contribution, such as data collection, analysis, or writing, and should not include vague or general statements. If no contributions are provided, this field should be set to null.\",\n",
                "      \"anyOf\": [\n",
                "        {\n",
                "          \"type\": \"array\",\n",
                "          \"items\": {\n",
                "            \"type\": \"string\"\n",
                "          }\n",
                "        },\n",
                "        {\n",
                "          \"type\": \"null\"\n",
                "        }\n",
                "      ]\n",
                "    },\n",
                "    \"authors\": {\n",
                "      \"description\": \"The \\\"authors\\\" field must contain an array of strings, each representing the full name of an author associated with the dataset or research work. The names should be formatted as \\\"First Last\\\" without any titles or affiliations included. This field is required to accurately attribute contributions to the respective authors in the context of the dataset.\",\n",
                "      \"anyOf\": [\n",
                "        {\n",
                "          \"type\": \"array\",\n",
                "          \"items\": {\n",
                "            \"type\": \"string\"\n",
                "          }\n",
                "        },\n",
                "        {\n",
                "          \"type\": \"null\"\n",
                "        }\n",
                "      ]\n",
                "    },\n",
                "    \"title\": {\n",
                "      \"description\": \"The \\\"title\\\" field must contain the complete title of the document or work being referenced. It should be a string that accurately reflects the main subject or focus of the content, without any abbreviations or alterations. Ensure that the title is extracted as it appears in the source material, maintaining proper capitalization and punctuation.\",\n",
                "      \"anyOf\": [\n",
                "        {\n",
                "          \"type\": \"string\"\n",
                "        },\n",
                "        {\n",
                "          \"type\": \"null\"\n",
                "        }\n",
                "      ]\n",
                "    },\n",
                "    \"methodology\": {\n",
                "      \"type\": \"string\",\n",
                "      \"description\": \"The \\\"methodology\\\" field must describe the specific research methods, experimental designs, techniques, or approaches used to conduct the study. This includes data collection procedures, model architectures, training strategies, evaluation protocols, and any novel technical contributions to the research process itself.\"\n",
                "    },\n",
                "    \"limitations\": {\n",
                "      \"type\": \"array\",\n",
                "      \"description\": \"The \\\"limitations\\\" field must contain an array of strings listing the acknowledged weaknesses, constraints, and boundaries of the research. This includes scope restrictions, potential biases in data or methods, scenarios where the approach may fail, computational requirements, and areas identified for future improvement.\",\n",
                "      \"items\": {\n",
                "        \"type\": \"string\"\n",
                "      }\n",
                "    },\n",
                "    \"topics\": {\n",
                "      \"type\": \"array\",\n",
                "      \"description\": \"The \\\"topics\\\" field must list the main subject areas, research themes, and technical domains covered by the paper. This includes specific tasks addressed (e.g., question answering, code generation), model types (e.g., transformer, diffusion), and application areas (e.g., healthcare, robotics).\",\n",
                "      \"items\": {\n",
                "        \"type\": \"string\"\n",
                "      }\n",
                "    }\n",
                "  },\n",
                "  \"required\": [\n",
                "    \"affiliation\",\n",
                "    \"contributions\",\n",
                "    \"authors\",\n",
                "    \"title\",\n",
                "    \"methodology\",\n",
                "    \"limitations\",\n",
                "    \"topics\"\n",
                "  ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "5. Deploy and copy the **Serving Endpoint Name**.\n",
                "6. Paste it below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dbutils.widgets.text(\"kie_endpoint\", \"agents_arxiv-kie\", \"KIE Endpoint (After Creation)\")\n",
                "os.environ[\"KIE_ENDPOINT\"] = dbutils.widgets.get(\"kie_endpoint\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Create Evaluation Dataset\n",
                "Create the table `arxiv_demo.main.eval_questions` for use in the KA Evaluation UI."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the script logic\n",
                "# We can just run the script file as a command if in the repo\n",
                "%run scripts/create_eval_table"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Run the App\n",
                "Run the Streamlit app directly in this notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from streamlit.web.cli import main\n",
                "import sys\n",
                "\n",
                "# Fake args for streamlit\n",
                "sys.argv = [\"streamlit\", \"run\", \"app/main.py\"]\n",
                "main()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}